{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pixels\n",
       "0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the data into labels and features\n",
    "labels, data = data['emotion'], data.drop(['emotion','Usage'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a82e10320>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXvjGNIgXvjEN4oVvTIN44RvTIP8GhAeK/H0M050AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a83001400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking in the pixels of image i.e the features as a numpy array\n",
    "pixel_data = data.values\n",
    "pixel_data = pixel_data.astype(str)\n",
    "pixel_data = np.core.defchararray.rsplit(pixel_data, sep=None, maxsplit=None)\n",
    "\n",
    "# Visualizing the images using matplotlib\n",
    "first_row = np.asarray((pixel_data)[0][0]).astype('float32')\n",
    "vis_image = first_row.reshape(48,48)\n",
    "plt.imshow(vis_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reshaping Data\n",
    "\n",
    "This entire part focuses on reshaping the data to be fed into the placeholder tensors which are defined further in the code. This was probably one of the hardest parts of the code and had to be done after reading an extensive amount of numpy documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 48, 48, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining an empty list to hold values\n",
    "image_data = []\n",
    "# Here we parse through each element in pixel_data, extract and reshape the images and then finally resize it appropriately\n",
    "for i in range(len(pixel_data)):\n",
    "    row_data = np.asarray((pixel_data)[i][0]).astype('float32')\n",
    "    # Normalizing the data to a value between 0 and 1\n",
    "    row_data = row_data/255.0\n",
    "    reshaped_row_data = row_data.reshape(48,48)\n",
    "    image_data.append(reshaped_row_data)\n",
    "big_data = np.vstack(image_data)\n",
    "big_data.shape\n",
    "\n",
    "'''\n",
    "The shape of the array holding image data should be of size (number of image samples, height, width, \n",
    "number of input color channels)\n",
    "'''\n",
    "final_data = big_data.reshape(35887,48,48,1)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's set aside the image data and process the labels. Since labels are categorical variables we can one-hot-encode them\n",
    "encoded_labels = pd.get_dummies(labels)\n",
    "encoded_labels = encoded_labels.as_matrix()\n",
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now to split the data into training and validation split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(final_data, encoded_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,shape=[None,image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "         \n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,shape=[None, n_classes], name='y')\n",
    "     \n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=None, name='keep_prob')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = int(x_tensor.get_shape().as_list()[3])\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],depth,conv_num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # Convolutional layer + Maxpooling layer\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1],1], strides=[1, pool_strides[0], pool_strides[1], 1], padding=\"SAME\")\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Storing the dimensions of the tensor as a list into a variable to make it easily accessible.\n",
    "    list_dim = x_tensor.get_shape().as_list()\n",
    "    # Here the reshape function returns a tensor of reduced dimensionality, 2-D in this case with batch size as none.\n",
    "    flattened_tensor = tf.reshape(x_tensor, [-1, list_dim[1]*list_dim[2]*list_dim[3]])\n",
    "    return flattened_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # Here I'm defining separate weights and bias for the fully connected layer..these weights live inside of this function.\n",
    "    weights_full = tf.Variable(tf.truncated_normal([dimension, num_outputs], stddev = 0.05))\n",
    "    bias_full = tf.Variable(tf.zeros([num_outputs]))\n",
    "    # This is a regular hidden layer with weights and biases.\n",
    "    fully_connected = tf.add(tf.matmul(x_tensor, weights_full),bias_full)\n",
    "    fully_connected = tf.nn.relu(fully_connected)\n",
    "    return fully_connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    new_dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # This is very similar to the fully connected layer.\n",
    "    weights_out = tf.Variable(tf.truncated_normal([new_dimension, num_outputs], stddev = 0.05))\n",
    "    bias_out = tf.Variable(tf.zeros([num_outputs]))\n",
    "    #Implementing the output layer.\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights_out), bias_out)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\" \n",
    "    # The first three convolutional + maxpooling layers\n",
    "    neural_net = conv2d_maxpool(x, 54, (5,5), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (1,1), (2,2), (4,4), (2,2))\n",
    "    neural_net = conv2d_maxpool(neural_net, 72, (2,2), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # Layer to flatten the tensor from the convolutional layers\n",
    "    neural_net = flatten(neural_net)\n",
    "    \n",
    "    # Two fully connected layers with dropout\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    \n",
    "    # Layer for output\n",
    "    neural_net = output(neural_net, 7)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return neural_net\n",
    "\n",
    "##########Building the Neural Network##########\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((48, 48, 1))\n",
    "y = neural_net_label_input(7)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Two functions to train the network and print stats \n",
    "def train_network(session, optimizer, keep_probability, features,labels):\n",
    "    \n",
    "    # Optimizing the neural net\n",
    "    session.run(optimizer, feed_dict={x:features,y:labels, keep_prob:keep_probability})\n",
    "    pass\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \n",
    "    # Calculating loss and validation accuracy\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    print(\"loss\", loss, \"valid_acc\", valid_acc)\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining the hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a float into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    918\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 919\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2472\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2561\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[0;32m-> 2562\u001b[0;31m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   2563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a float into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0d638be21907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-5db4ded77b18>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(session, optimizer, keep_probability, features, labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Optimizing the neural net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_probability\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    920\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 922\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a float into a Tensor."
     ]
    }
   ],
   "source": [
    "# Finally let's get training \n",
    "print(\"Training on the data\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_network(sess,optimizer, keep_prob, data_train, labels_train)\n",
    "    \n",
    "    print('Epoch {:>2}:  '.format(epoch + 1), end='')\n",
    "    print_stats(sess, data_train, labels_train, cost, accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
